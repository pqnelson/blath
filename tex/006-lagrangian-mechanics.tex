\title{Summary of Lagrangian Mechanics}
\author{Alex Nelson\footnote{Email: \texttt{pqnelson@gmail.com}\qquad
  Updated: \today}}
\date{August 22, 2022}
\maketitle

\section{Deriving Newton's Laws of Motion}
\M
We usually set up the equations of motion directly using Newton's second
Law of motion. But we may conveniently note: many forces may be written
as the gradient of a potential energy function $\vec{F}=-\vec{\nabla}U$.
Then the equations of motion look like
\begin{equation}
m\frac{\D}{\D t}\vec{v} = -\vec{\nabla}U
\end{equation}
where $\vec{v}$ is the velocity vector for the body.

The strategy is to derive the equations of motion from a ``generating
function''-type object called the Lagrangian.

\begin{definition}[Provisional definition --- technically incorrect]
The \define{Lagrangian} for a point-particle is a function of its
position $\vec{q}$ and velocity $\vec{v}$ (treated as independent
variables) and potential energy $U(\vec{q})$ as
\begin{equation}
L(\vec{q},\vec{v}) := \frac{1}{2}mv^{2} - U(\vec{q}).
\end{equation}
(Caution: this is a provisional definition, and works only in
Cartesian/rectangular coordinates.)
\end{definition}

\N{Euler-Lagrange Equations}
We can now write down the equations of motion using derivatives of the
Lagrangian. These seem a bit magical, but we will verify we can recover
Newton's equations of motion, then discuss the derivation of these
equations.

The equations of motion are obtained from the Euler-Lagrange equations:
\begin{equation}
\frac{\D}{\D t}\frac{\partial L}{\partial v^{j}} - \frac{\partial L}{\partial q^{j}} = 0
\end{equation}
where $q^{j}$ is the $j$-component of position, $v^{j}$ is the
$j$-component of velocity, and we stress we assume position and velocity
are independent. So in particular, we assume
\begin{equation}
\frac{\partial q^{k}}{\partial v^{j}} = \frac{\partial v^{j}}{\partial q^{k}} = 0
\end{equation}
for all $j$, $k$.

\M
If we plug in our definition of the Lagrangian for a point-particle, we
find
\begin{subequations}
\begin{align}
\frac{\partial L}{\partial v^{j}}
&= \frac{\partial (mv^{2}/2)}{\partial v^{j}} - \frac{\partial U(q)}{\partial v^{j}}\\
&= \frac{\partial (mv^{2}/2)}{\partial v^{j}} - 0\\
&= \frac{m}{2}\frac{\partial (v^{2})}{\partial v^{j}}\\
&= \frac{m}{2}2 v^{j} = mv^{j}
\end{align}
\end{subequations}
Hence the first term in the Euler-Lagrange equations gives us the
right-hand side to Newton's second law (the ``$ma$'' part):
\begin{equation}
\frac{\D}{\D t}\frac{\partial L}{\partial v^{j}} =
\frac{\D}{\D t}(m v^{j}).
\end{equation}
So far, so good.

\begin{remark}[Generalized momentum]
We define the quantity
\begin{equation}
p_{j} := \frac{\partial L}{\partial v^{j}}
\end{equation}
to be the \define{Generalized Momentum} (or \define{Conjugate Momentum})
associated to the position variable $q^{j}$.
\end{remark}

\begin{remark}[Indices, upstairs and downstairs]
It may appear seemingly random when an index is superscripted
(``upstairs'') like position $q^{j}$, or when an index is subscripted
(``downstairs'') like momentum $p_{j}$. Although beyond the scope of
these notes, they encode how the quantity behaves under a change of
coordinates: do we multiply by the Jacobian matrix or the inverse matrix?
It coincidentally allows for using Einstein summation
convention\footnote{Quantum physicists blunder this elegant convention
with their obtuse Euclidean summation convention, where any repeated
indices are summed over, the distinction between upstairs and downstairs
becomes irrelevant, and confusion reigns supreme.}, where
an index which appears upstairs in one quantity --- when multiplied by a
quantity with the same index appearing downstairs --- results in an
implicit sum over that index; e.g., $v^{j}p_{j} = \sum_{j}v^{j}p_{j}$.
We will be explicit about sums in our notes.
\end{remark}

\M
The other term contributes, as one might expect, the only remaining
contribution to Newton's second law: the force term. We can compute it
directly as
\begin{subequations}
\begin{align}
\frac{\partial L}{\partial q^{j}}
&= \frac{\partial (mv^{2}/2)}{\partial q^{j}} - \frac{\partial U(q)}{\partial q^{j}}\\
&= 0 - \frac{\partial U(q)}{\partial q^{j}}\\
&= -(\vec{\nabla}U)^{j}.
\end{align}
\end{subequations}
This is precisely the $j$-component of the gradient of the potential energy.

\N{Recovering Newton}
When we combine our results together, we find the Euler-Lagrange
equations for a point-particle give us
\begin{equation}
\frac{\D}{\D t}(m v^{j}) - (-\vec{\nabla}U)^{j} = 0.
\end{equation}
This is Newton's second law for a point-particle experiencing some
``external potential''.

However, we have been a bit stringent. We have defined the Lagrangian
\emph{for a point-particle} --- specifically \emph{in Cartesian coordinates}.
The reader may consider what the kinetic energy looks like in spherical
coordinates; as a hint, recall
\begin{equation}
\D\vec{r} = \D r\,\hat{r} + r\,\D\theta\,\hat{\theta} + r\sin(\theta)\,\D\varphi\,\hat{\varphi}.
\end{equation}
(Divide through by $\D t$ gives the velocity vector in spherical
coordinates.)
In particular, the Lagrangian should be
\begin{equation}
L(\vec{q},\vec{v}) = K(\vec{q},\vec{v}) - U(\vec{q},\vec{v})
\end{equation}
the difference between the kinetic energy $K$ and potential energy
$U$. \textbf{This is the correct definition for the Lagrangian of a\ point-particle.}
If we work through the computation using spherical coordinates, we will
find appropriate pseudoforce contributions computed automatically.

\N{Multiple bodies}
We can extend our definition from one point-particle to several. We just
need to include a kinetic energy term for each point-particle, and the
potential energy from interactions among the point-particles. The
Euler-Lagrange equations remain the same, we just have to compute them
for each point-particle. So, in summary, our Lagrangian will look like:
\begin{equation}
L = \left(\sum^{N_{\text{bodies}}}_{n=1}K(\vec{q}^{(n)},\vec{v}^{(n)})\right)
-U(\vec{q}^{(1)},\dots,\vec{q}^{(N)},\vec{v}^{(1)},\dots,\vec{v}^{(N)})
\end{equation}
where all the potential energy terms are ``swept under the rug'' of the
single term $U$.

\N{Beyond Newton}
We can use the Euler-Lagrange equations for situations where Newton's
second Law no longer holds. For example, we can find the equations of
motion for particles obeying special relativity with the Lagrangian
approach.

We can also generalize from one point-particle to fields, like the
electromagnetic field or gravitation or nuclear forces. This is done in
graduate school.

\section{Derivation of Euler-Lagrange Equations}

\M
The derivation of the Euler-Lagrange equations requires use of
``variational calculus''. Although beyond the scope of our notes, we
should think of variational calculus as one possible way to generalize
calculus to infinite dimensions, specifically to ``functions'' on the
space of [continuous] paths on a given space (usually $\RR^{n}$ for
fixed $n$).

Specifically, we will consider the space of paths (on a space $E$) who
start at the same point $\vec{q}_{0}$ and end at the same point
$\vec{q}_{1}$. Let us denote this space by
\begin{equation}
C(E; \vec{q}_{0},\vec{q}_{1}) = \{\gamma\colon[0,1]\to E\mid\gamma(0)=\vec{q}_{0},\gamma(1)=\vec{q}_{1},\gamma~\mbox{continuous}\}.
\end{equation}
We will study \define{Functionals}, mappings of the form
\begin{equation}
F\colon C(E; \vec{q}_{0},\vec{q}_{1})\to\RR
\end{equation}
and try to do calculus with these functionals. At least differentiation
(so as to find the critical points/``paths'' of the functional, leading
to where it takes extreme values). \textbf{Notation:} if $F$ is a
functional, then we write $F[\varphi]$ with square brackets to emphasize
it's a functional (as opposed to using parentheses).

How does this work? Well, recall one intuitive approach to
differentiation is by means of ``infinitesimals'' --- non-zero numbers
$\varepsilon>0$ such that $\varepsilon^{2}=0$. We then define the
derivative of a function as
\begin{equation}
f(x + \varepsilon) = f(x) + \varepsilon f'(x) 
\end{equation}
the coefficient of $\varepsilon$. For multivariable calculus, we can do
likewise, but now need to use a vector of infinitesimals and sum over
its components
\begin{equation}
  \begin{split}
f(\vec{x} + \vec{\varepsilon}) &= f(\vec{x}) + \vec{\varepsilon}\cdot\vec{\nabla}f(\vec{x})\\
&= f(\vec{x}) + \sum^{n}_{j=1}\varepsilon^{j}\frac{\partial}{\partial x^{j}}f(\vec{x}).
  \end{split}
\end{equation}

We pretend we can do likewise, just replace the summation with
integrals, and have for our function $F$ its variational derivative defined by
\begin{equation}
F[q + \varepsilon\,\delta q] = F[q] + \int^{1}_{0}\delta
q(t)\frac{\delta F[q]}{\delta q}\,\D t.
\end{equation}
Here the index variable $j$ is replaced by the continuous parameter $t$,
the vector of infinitesimals $\vec{\varepsilon}$ is replaced by the
``infinitesimal deformation to the path'' $\delta q$, and the vector
$\vec{x}$ is replaced by the path $q(t)$.

\N{Important: boundary contributions}
In physics, we usually care about deformations $\delta q$ which are such
that $(q + \delta q)(0) = q(0)$ and $(q + \delta q)(1)=q(1)$. That is to
say, the boundary contributions from the variation must be zero. Besides
this technical condition, the variation is completely arbitrary. 

\begin{example}
For physics, we usually care about functionals which are integrals of
some kind. For example,
\begin{equation}
F[q] = \int^{1}_{0}q(t)^{2}\,\D t.
\end{equation}
We find then that
\begin{subequations}
\begin{align}
F[q + \delta q] &= \int^{1}_{0}(q(t) + \delta q(t))^{2}\,\D t\\
&= \int^{1}_{0}(q(t)^{2} + 2q(t)\,\delta q(t) + [\delta q(t)]^{2})\D t\\
&= \int^{1}_{0}q(t)^{2}\,\D t + \int^{1}_{0}2q(t)\,\delta q(t)\,\D t +
\mathcal{O}((\delta q)^{2})\\
&= F[q] + \int^{1}_{0}2q(t)\,\delta q(t)\,\D t +
\mathcal{O}((\delta q)^{2}).
\end{align}
\end{subequations}
We throw away the higher-order terms involving quadratic orders of the
deformation path $(\delta q)^2$ and comparing this to the definition
given above, we find
\begin{equation}
\frac{\delta F}{\delta q} = 2q(t).
\end{equation}
\end{example}

\begin{remark}
It is important to observe the variational derivative gives us a
\emph{function}, not a number. This is analogous to a component of a
gradient. After all, we usually care about the \emph{variation} of a
functional, analogous to the \emph{differential} of a function, which
we define as
\begin{equation}
F[q + \delta q] = F[q] + \underbrace{\delta F[q, \delta q]}_{\text{variation}}.
\end{equation}
Levereging the hell out of our analogy with multidimensional calculus,
we will consider a path $q_{cr}$ to be a ``critical path'' if, for any
deformation $\delta q$, we have
\begin{equation}
F[q_{cr} + \delta q] = F[q_{cr}].
\end{equation}
This is completely analogous to the notion of a ``critical point''.
\end{remark}

\N{Action}
We now have developed sufficient infrastructure to derive the
Euler-Lagrange equations. The functional we work with is defined as the
\define{Action}, which is just the integral of the Lagrangian
\begin{equation}
S[\vec{q},\vec{v}] = \int^{t_{1}}_{t_{0}}L(\vec{q},\vec{v})\,\D t
\end{equation}
where we have fixed (but arbitrary) endpoints
$\vec{q}(t_{0})=\vec{q}_{0}$ and $\vec{q}(t_{1})=\vec{q}_{1}$.

The Euler-Lagrange equations are derived from the action by demanding
its variation (with respect to arbitrary deformation to the path)
vanish. There is some subtlety here, though, specifically from using the
time derivative of the path deformation as the deformation to the
velocity:
\begin{equation}
S[\vec{q} + \delta\vec{q}, \vec{v} + \frac{\D}{\D t}\delta\vec{q}]
= S[\vec{q},\vec{v}].
\end{equation}
Why on Earth should we believe this? Well, we can compute it directly
\begin{subequations}
\begin{align}
S[\vec{q} + \delta\vec{q}, \vec{v} + \frac{\D}{\D t}\delta\vec{q}]
&=\int^{t_{1}}_{t_{0}} L(\vec{q} + \delta\vec{q}, \vec{v} + \frac{\D}{\D t}\delta\vec{q})\,\D t\\
&=\int^{t_{1}}_{t_{0}}\left(
L(\vec{q},\vec{v}) + \sum^{n}_{j=1}\delta q^{j}\frac{\partial L(\vec{q},\vec{v})}{\partial q^{j}}
+\frac{\D}{\D t}\delta q^{j}\cdot \frac{\partial L(\vec{q},\vec{v})}{\partial v^{j}}
\right)\,\D t\\
&=\int^{t_{1}}_{t_{0}}L(\vec{q},\vec{v})\,\D t +
\int^{t_{1}}_{t_{0}}\left(\sum^{n}_{j=1}\delta q^{j}\frac{\partial L(\vec{q},\vec{v})}{\partial q^{j}}
+\frac{\D}{\D t}\delta q^{j}\cdot \frac{\partial L(\vec{q},\vec{v})}{\partial v^{j}}
\right)\,\D t
\end{align}
\end{subequations}
We're half-way done.

The trick is to use integration-by-parts to rewrite
\begin{equation}
 \int^{t_{1}}_{t_{0}}
  \frac{\D}{\D t}\delta q^{j}\cdot \frac{\partial L(\vec{q},\vec{v})}{\partial v^{j}}\,\D t
  = (\mbox{boundary terms}) -
  \int^{t_{1}}_{t_{0}}
  \delta q^{j}\cdot \frac{\D}{\D t}\frac{\partial L(\vec{q},\vec{v})}{\partial v^{j}}\,\D t.
\end{equation}
The boundary terms involve multiplication by $\delta q$, which vanish on
the boundary, so we see they equal zero. We then have
\begin{equation}
S[\vec{q} + \delta\vec{q}, \vec{v} + \frac{\D}{\D t}\delta\vec{q}]
=\int^{t_{1}}_{t_{0}}L(\vec{q},\vec{v})\,\D t +
\int^{t_{1}}_{t_{0}}\sum^{n}_{j=1}\delta q^{j}\underbrace{\left(\frac{\partial L(\vec{q},\vec{v})}{\partial q^{j}}
-\frac{\D}{\D t}\frac{\partial L(\vec{q},\vec{v})}{\partial v^{j}}
\right)}_{\text{Euler-Lagrange equations}}\,\D t.
\end{equation}
If we demand the physical trajectory is a critical path, then
necessarily the parenthetic term on the right-hand side must vanish;
that is to say, the Euler-Lagrange equations must hold.
Thus we have derived the Euler-Lagrange equations by finding the
critical paths for the action functional.

\N{Interpreting Lagrangian Approach}
We should interpret the Lagrangian approach to physics as ``operating
on'' the space of possible trajectories. We then have a ``stud detector''
[the action functional] which tells us when a possible trajectory is
physically realized. This is completely different than Newtonian
physics, which works with free-body diagrams and force vectors directly.

\N{Best way to study Lagrangian Mechanics}
In my humble opinion, the best way to study Lagrangian mechanics is by
means of several books and writing software, as well as studying
variational calculus. Good books on the variational calculus include:
\begin{enumerate}
\item Gilbert Ames Bliss, \emph{Calculus of Variations}. Corus
  Mathematical Monographs, 1925. (A very gentle introduction, useful
  supplement to Abbelson and friends's \emph{SICM} below.)
\item Israel Gelfand and S.V.~Fomin's
  \emph{Calculus of Variations}. Dover publications. (This is the alpha
  and omega text on the subject, very rigorous, Russian-style.)
\end{enumerate}
For Lagrangian mechanics, I enthusiastically suggest
Gerald Jay Sussman and Jack Wisdom's \emph{Structure and Interpretation of Classical Mechanics}
(sometimes just abbreviated as ``\emph{SICM}'')
which works through analytical mechanics using the Scheme programming
language. This fleshes out all the subtleties which most books skim
over.

On the physics side, there are no \textbf{good} books, I am afraid. The
standard textbooks recommended (each with positive and negative
qualities):
\begin{enumerate}
\item John Taylor, \emph{Classical Mechanics}. (Spends first quarter of
the book reviewing Newtonian mechanics, the second quarter on Lagrangian
mechanics, and the last half on ``further topics''.)
\item Herbert Goldstein, \emph{Classical Mechanics}. Be sure to get
the \textbf{Second edition}, which discusses rotational mechanics using spinors.
\item Hand and Finch, \emph{Analytical Mechanics}. CUP. (This was used
at UC Davis for a while; assumes reader has worked through something
like Kleppner and Kolenkow's \emph{Introduction to Mechanics}.)
\item Landau and Lifshitz, \emph{Mechanics}. (Derives physics from
``first principles'', stipulating the principle of stationary action
$\delta S=0$ determines physical paths. Extraordinarily elegant, terse, beautiful.)
\end{enumerate}
Sometimes Arnold's \emph{Mathematical Methods for Classical Mechanics}
is added, more as a joke; it's a beautiful book, for graduate-level
mathematicians, who want to learn more about classical mechanics in
terms that a mathematician can understand.

\begin{remark}[On writing software]
When I say, ``Writing software'', I have in mind writing a library in
some Lisp (any Scheme is fine) which allows you to write code which
resembles the equations you are evaluating. For example,
\begin{Verbatim}
(integrate (lambda (x) ...) :d x :from 0 :to 1)
\end{Verbatim}
Such code may be found in Sussman and Wisdom's book. Its implementation
details are not adequately discussed, so a good book on numerical
analysis may be fruitful (e.g., Burden and Faires's \emph{Numerical Analysis}).

Lisp allows you to implement your own programming language, in the sense
that: after you have transformed input into an abstract syntax tree,
that tree is a Lisp S-expression. Lisp ``jumps the queue'' to that
point, the exciting part where you implement your own nontrivial bits of
the language. This is where macros come in: a Lisp macro extends the
``pool of primitive parts'' to the language.

You should write semantic code to the effect of ``\verb#(variation functional)#''
or whatever. When in doubt about naming things,
consult \emph{Mathematica} and see what it does. Half the
time, \emph{Mathematica} has a sensible approach; the other half, it's
so obviously terrible, you have a good clue ``what not to do''.

Also, be sure to write unit tests (which act as examples for how to use
the code \emph{and} as regression ``safe guards'' to ensure the
functions behave as expected), documentation strings, add contracts to
enforce assumptions, and so on.

I cannot recommend enough the books Gerald Jay Sussman has written,
from \emph{Structure and Interpretation of Computer Programs} to his
most recent \emph{Software Design for Flexibility}.
\end{remark}

\section{Miscellaneous Topics}

\N{Constraints}
Often, one will want to consider situations where a body may move only
over a certain region (for example, on a sphere, where the radius is
fixed). We handle this situation in Lagrangian mechanics by adding to
the Lagrangian a constraint term with a Lagrange multiplier. The
Lagrangian then looks like:
\begin{equation}
L(\vec{q},\vec{v}) = K(\vec{q},\vec{v}) - V(\vec{q},\vec{v}) + \lambda f(\vec{q}).
\end{equation}
The variation of the action then becomes
\begin{equation}
\int^{t_{2}}_{t_{1}}\delta L(\vec{q},\vec{v})\,\D t
= \int^{t_{2}}_{t_{1}}\sum^{n}_{k=0}\left(
\frac{\partial L}{\partial q^{k}}
\frac{\D}{\D t}\frac{\partial L}{\partial v^{k}}
+ \lambda\frac{\partial f}{\partial q^{k}}
\right)\delta q^{k}\,\D t.
\end{equation}
We can add as many constraints as we would like, we just need to
multiply by a distinct Lagrange multiplier:
\begin{equation}
L(\vec{q},\vec{v}) = K(\vec{q},\vec{v}) - V(\vec{q},\vec{v}) + \sum^{C}_{j=0}\lambda_{j} f_{j}(\vec{q}).
\end{equation}
The equations of motion generalize accordingly.

There is a more general need to study constraints in gauge theory, which
again lies beyond the scope of this note. Basically, symmetries may
appear in the Lagrangian, which takes the form of redundant
variables. But then not all variables are independent of each other, and
these extra relations (``constraints'') occur.

\N{Rotational Mechanics}
Although I neglected to mention this fact, it is true Lagrangian
mechanics is coordinate independent. This is useful when dealing with
rotational mechanics. Every textbook belabours this point, so I just
mention it in passing.

\N{Hamiltonian Mechanics}
Although useful, the Lagrangian may be transformed into the
Hamiltonian. Intuitively, this is something analogous to a ``change of
variables'' from velocity $\vec{v}$ to momenta $\vec{p}$. We then can
write the velocities as a function of position and momenta
\begin{equation}
\vec{v} = \vec{v}(\vec{q},\vec{p})
\end{equation}
and the Hamiltonian is then defined using the Legendre transform
\begin{equation}
  \begin{split}
  H(\vec{q},\vec{p}) &= \vec{p}\cdot\vec{v}(\vec{q},\vec{p})
  - L(\vec{q}, \vec{v}(\vec{q},\vec{p}))\\
  &= K(\vec{q}, \vec{v}(\vec{q},\vec{p})) + U(\vec{q}, \vec{v}(\vec{q},\vec{p})).
  \end{split}
\end{equation}
For non-relativistic physics, the Hamiltonian is precisely the total
energy of the system. Hence, for conservative forces, the Hamiltonian is
constant. Geometrically, this means we work on a subspace of the
\define{Phase Space} --- the $6N$-dimensional mathematical space
describing the configuration of bodies by their possible positions and
momenta.

There's a lot more that could be said of Hamiltonian mechanics. Not only
can we derive the equations of motion similarly, but there's an entirely
new ``mathematical apparatus'' called the Poisson bracket. This is a
sort of mapping which eats two functions on the phase space, and
produces a new function on the phase space. If we fix the first argument
to be the Hamiltonian, then it gives us the time derivative of the
second argument. There is a lot which may be said of the Poisson
bracket, since we could use it to study symmetry transformations (which
is done in the Hamiltonian formulation of gauge theory, relating the
study of Lie algebras to mechanics). The best/only textbook on this is
Henneaux and Teitelboim's \emph{Quantization of Gauge Systems}; the
first dozen chapters (or so) are purely about studying gauge symmetries
of classical systems.

\N{Geometry of Mechanics}
We can meaningfully study the geometry of Lagrangian (and especially
Hamiltonian) mechanics. This is a rich field of differential geometry,
called \define{Symplectic Geometry}. It's especially beautiful, and well
beyond the scope of our note. Here we study the geometry of the phase
space (the $6N$-dimensional mathematical space of positions and
momenta), and various gadgets useful for doing physics. Symplectic
geometry tends to ``eat itself'', as most fields do, making direct
connections to physical problems opaque at times.

But we can use symplectic geometry to leverage quite a bit of
information when quantizing systems. In fact, we have an entirely
geometric route to quantization! This may be found in the
comprehensive textbook \emph{Geometric Quantization} by Woodehouse.

\N{Miscellaneous Analytical Mechanics}
There are other avenues of study, within analytical mechanics, once we
have established the Hamiltonian formalism. For example, we can consider
different coordinates (which may be useful for different families of
physical problems).

The Hamilton--Jacobi equation works in a set of coordinates where the
momentum variables are replaced by partial derivatives with respect to
the action:
\begin{equation}
p_{j} = \frac{\partial S(\vec{q},t)}{\partial q^{j}}.
\end{equation}
How on Earth is this possible? We are a bit sloppy here, to be frank; we
should have written
\begin{equation}
S(\vec{q},t;\vec{q}_{0},t_{0}) = \int^{t}_{t_{0}} L(\gamma(\tau;\cdot),
\dot{\gamma}(\tau;\cdot))\,\D\tau
\end{equation}
where $\gamma=\gamma(\tau;t,t_{0},\vec{q},\vec{q}_{0})$ is a trajectory
such that $\gamma|_{\tau=t_{0}}=\vec{q}_{0}$ and $\gamma|_{\tau=t}=\vec{q}$.
Here $\dot\gamma$ is the time-derivative (with respect to $\tau$) of
this $\gamma$. Also note we use parentheses $S(\dots)$ because the
action is now a function, not a functional. We then have the
Hamilton--Jacobi equation 
\begin{equation}
-\frac{\partial S}{\partial t} = H\left(\vec{q},\frac{\partial S}{\partial\vec{q}},t\right).
\end{equation}
This is a first-order, nonlinear partial differential equation.

There are other, more exotic approaches to analytical
mechanics. Routhian mechanics is a famous one, a hybrid of Lagrangian
and Hamiltonian mechanics. Valdmir Arnold discusses them extensively in
his confusingly entitled book \emph{Mathematical aspects of classical and celestial mechanics},
which should not be mistaken for his other book \emph{Mathematical Aspects of Classical Mechanics}.

\N{Continuum Mechanics}
The final direction I will mention briefly, if instead of considering a
point particle (or a system of finitely many point-particles), what if
we consider taking the continuum limit and describe a \emph{region} (for
example, a pencil eraser)? This gives us continuum mechanics, which
covers fluid mechanics and elastic bodies. Usually these fields are
taught in the engineering departments, and closely coupled with
finite-element numerical methods\footnote{Finite-element methods usually
take some choice of basis functions for the space of functions
containing the solution to a particular PDE, then ``truncates'' the set
of basis functions to a particular order. Then an approximate solution
may be obtained as a linear combination of the ``truncated set'' of
basis functions, and the partial differential equation becomes either a
linear equation or an algebraic equation.}.
This is particularly challenging, and I know no good text for continuum
mechanics \emph{in general}, from the analytical mechanics perspective.

\N{Fields}
Physical fields may be studied using Lagrangian mechanics. Greiner's
book on quantum field theory works through the derivations very
carefully and explicitly. Basically, we can consider a scalar field as a
system of point-masses connected by means of springs. Take the continuum
limit of the number of point-masses, and we obtain a scalar field (like
temperature). We can do something analogous for the electromagnetic
4-potential, and any field. This is studied in graduate school, usually
in quantum field theory courses. There are few good texts on the subject
of classical field theory, Walter Thirring's \emph{Classical Mathematical Physics}
is an excellent one.